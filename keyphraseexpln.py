import pickle
import numpy as np
from keyphrase_vectorizers import KeyphraseCountVectorizer
from keybert import KeyBERT
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import KNeighborsClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.linear_model import LogisticRegression
from sklearn.multioutput import MultiOutputClassifier, ClassifierChain
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import CountVectorizer
from xxhash import xxh3_64 as xhash
import heavenCashe
import yake
from rake_nltk import Rake
from flair.embeddings import TransformerDocumentEmbeddings, TransformerWordEmbeddings
from sklearn.preprocessing import OneHotEncoder
from flair.data import Sentence


class CAKE:
    def __init__(self, label_names, top_n, model_path=None):
        if model_path is not None:
            kw = TransformerDocumentEmbeddings(model_path, layers='-1')
            self.embedding_model = KeyBERT(model=kw)
            self.model_type = 'Trained'
            self.crazy_emb = TransformerWordEmbeddings(model_path, layers='-1', layer_mean=False, use_context=True)
        else:
            self.embedding_model = KeyBERT()
            self.model_type = 'Sbert'


        self.model_path = model_path
        self.label_names = label_names
        self.top_n = int(top_n)
        self.backupvectorizer = CountVectorizer(ngram_range=(2, 2))
        self.pipeline = 'PatternRank'
        self.rank = 'classifier'
        self.threshold = 0
        self.empty_PR = KeyphraseCountVectorizer()
        self.rake = Rake()
        self.yake = yake.KeywordExtractor(n=3, windowsSize=1, top=25)
        self.label_emb = None
        self.token_lvl = None
        self.pattern_simple = False
        self.sudo_keyphrase_embed = False
        return

    # trains patternRank vectorizer on selection of train documents
    def train_vectorizer(self, docs):
        cashed = heavenCashe.hCasheobj(KeyphraseCountVectorizer().fit)
        self.pattern_vectorizer = cashed(docs)
        return

    # change parameters of cake for next run
    def change_para(self, top_n : float, pipeline, thres):
        self.top_n = int(top_n)
        self.pipeline = pipeline
        if type(self.pipeline) is tuple :
            self.backupvectorizer = CountVectorizer(ngram_range=pipeline)
        self.threshold = thres
        return

    def document_and_word_embed(self, docs, unique='a'):
        extract = heavenCashe.numpyDisksave(self.embedding_model.extract_embeddings)
        if self.model_type == 'Trained':
            doc_embeddings, word_embeddings = extract(docs=docs, vectorizer=self.pattern_vectorizer, vect_trained=True, stop_words=unique)
        else:
            doc_embeddings, word_embeddings = extract(docs=docs, vectorizer=self.pattern_vectorizer, vect_trained=True)
        self.doc_emb = doc_embeddings
        self.cand_emb = word_embeddings
        return

    #  make predictions using cosine similarity(does not handle multi-label)
    def predict_cosine(self, x):
        doc_embedding = self.embedding_model.model.embed(x)
        distances = cosine_similarity(doc_embedding, self.label_emb)
        predictions = np.argmax(distances, axis=1)

        if len(self.label_names) > 2:
            one = OneHotEncoder().fit(np.array(range(len(self.label_names))).reshape(-1,1))
            predictions = one.transform(predictions.reshape(-1, 1))
        return predictions

    # do predictions and calculate f1 score
    def calculate_f1(self, x, y):
        if self.rank == 'classifier':
            if self.model_type=='Trained':
                casher = heavenCashe.namedhCasheobj(self.model_path, self.rank)
                em = casher(self.embedding_model.model.embed)
                y_pred = self.cl.predict(em(x))
            else:
                y_pred = self.cl.predict(self.embedding_model.model.embed(x))
        elif self.rank == 'cosine':
            y_pred = self.predict_cosine(x)
        else:
            return np.nan

        return f1_score(y, y_pred, average='macro')

    # load dataset descriptions generated by chatGPT
    def load_descriptions(self):
        print(xhash(str(self.label_names)).intdigest())
        with open(f'descriptions/{xhash(str(self.label_names)).intdigest()}','rb') as f:
            desc = pickle.load(f)
        print(list(desc))
        return desc

    # load precomputed label embeddings
    def load_precomputed(self, dataname, distil):
        identifier = f'{dataname} {str(distil)}'
        print(xhash(identifier).intdigest())
        with open(f'token_generated/{xhash(identifier).intdigest()}', 'rb') as f:
            desc = pickle.load(f)
        print(desc.shape)
        return desc

    def train_cake(self, y, classifier_data_model='knn ', multi=False, calibration=None, chain=False, k=5):
        classifier = classifier_data_model.split(' ')[0]
        if classifier == 'cosine':
            self.avg_label_embedding(y)
            self.rank = 'cosine'
            return
        elif classifier == 'zero':
            des = self.load_descriptions()
            self.label_emb = self.embedding_model.model.embed(des)
            self.rank = 'cosine'
            return
        elif classifier == 'cosine_pre':
            emb = self.load_precomputed(classifier_data_model.split(' ')[1], classifier_data_model.split(' ')[2])
            self.label_emb = emb
            self.rank = 'cosine'
            return
        elif classifier== 'perdoc_cosine':
            self.avg_label_embedding(y)
            self.rank = 'perdoc_cosine'
            return
        elif classifier=='logistic':
            cl = LogisticRegression()
        elif classifier == 'tree':
            cl = DecisionTreeClassifier()
        elif classifier == 'bayes':
            cl = GaussianNB()
        elif classifier == 'svc':
            cl = LinearSVC()
            if calibration is None:
                calibration = 'isotonic'
        elif classifier == 'knn':
            cl = KNeighborsClassifier(k)
        else:
           cl = None

        if cl == None:
            if calibration is not None:  # and not (multi and chain):
                cl = CalibratedClassifierCV(cl, cv=3, method=calibration)
            if multi:
                if chain:
                    cl = ClassifierChain(cl, order='random')
                    # cl = VotingClassifier
                elif classifier != 'knn' or calibration is not None:
                    cl = MultiOutputClassifier(cl)
            self.rank = 'classifier'
            cl.fit(self.doc_emb, y)
            self.cl = cl
            return

    # average embeddings from train documents
    def avg_label_embedding(self, y):
        if len(self.label_names) == 2:
            label1 = np.average(self.doc_emb[y == 0], axis=0)
            label2 = np.average(self.doc_emb[y == 1], axis=0)
            self.label_emb = np.array([label1, label2])
        else:
            label_embeddings = list()

            for i, label in enumerate(self.label_names):
                label_embeddings.append(np.average(self.doc_emb[y.T[i] == 1], axis=0))
            self.label_emb = label_embeddings

    def backup_candidates(self, doc):
        self.backupvectorizer.fit([doc])
        candidates = self.backupvectorizer.get_feature_names_out()
        candidates = list(candidates)
        embed = self.embedding_model.model.embed(candidates)
        return candidates, embed

    # pattern rank candidate generation, simple version
    def Pattern_cand(self, doc):
        words = self.pattern_vectorizer.get_feature_names_out()
        doc_term_matrix = self.pattern_vectorizer.transform([doc])
        candidate_indices = doc_term_matrix.nonzero()[1]

        candidates = [words[index] for index in candidate_indices]
        candidate_embeddings = self.cand_emb[candidate_indices]

        if self.pattern_simple:
            self.empty_PR.fit([doc])
            words = self.empty_PR.get_feature_names_out()
            doc_term_matrix = self.empty_PR.transform([doc])
            candidate_indices = doc_term_matrix.nonzero()[1]
            if len(candidate_indices) > 0:
                candidates2 = [words[index] for index in candidate_indices]
                candidates.extend(candidates2)

                candidate_embeddings2 = self.embedding_model.model.embed(candidates2)
                candidate_embeddings = np.vstack((candidate_embeddings, candidate_embeddings2))

        return candidates, candidate_embeddings

    def single_pass(self, doc):
        if self.token_lvl is None:
            self.token_lvl = TransformerWordEmbeddings(self.model_path, layers='-1', layer_mean=False, use_context=True)
        doc = Sentence(doc)
        emb = self.token_lvl.embed(doc)
        token_emb = []
        tokens_textafdae = []
        for token in emb[0].tokens:
            token_emb.append(np.array(token.embedding.cpu()))
            tokens_textafdae.append(token.text)
        token_emb = np.array(token_emb)
        token_emb = np.lib.stride_tricks.sliding_window_view(token_emb, 2, axis=0)
        token_emb = np.mean(token_emb, axis=2)
        tokens_textafdae = np.lib.stride_tricks.sliding_window_view(tokens_textafdae, 2)
        candidates = [' '.join(i) for i in tokens_textafdae]
        return candidates, token_emb


    def get_candidates(self, doc):
        if self.pipeline == 'PatternRank':
            candidates, candidate_embeddings = self.Pattern_cand(doc)
        elif type(self.pipeline) is tuple:
            candidates, candidate_embeddings = self.backup_candidates(doc)
        elif self.pipeline == 'Rake':
            self.rake.extract_keywords_from_text(doc)
            candidates = self.rake.get_ranked_phrases()
            candidate_embeddings = self.embedding_model.model.embed(candidates)
        elif self.pipeline == 'Yake':
            temp = self.yake.extract_keywords(doc)
            candidates = [i for i, w in temp]
            candidate_embeddings = self.embedding_model.model.embed(candidates)
        elif self.pipeline == 'single':
            candidates, candidate_embeddings = self.single_pass(doc)
        else:
            candidates, candidate_embeddings = self.backup_candidates(doc)

        return candidates, candidate_embeddings

    def sudo_keyphrase_embedding(self, candidates, doc):
        candidate_embeddings = []
        for i, cand in enumerate(candidates):

            sent = (Sentence(f'{cand} is a keyphrase for document: {doc}'))
            embeddings = self.crazy_emb.embed(sent)
            cand_emb = []
            for token in embeddings[0].tokens[2:2 + len(cand.split(' '))]:
                cand_emb.append(np.array(token.embedding.cpu()))

            candidate_embeddings.append(np.mean(cand_emb, axis=0))

        candidate_embeddings = np.array(candidate_embeddings)
        return candidate_embeddings

    def perdoc_label_embed(self, doc):
        label_embeddings = []
        for i, label in enumerate(self.label_names):

            sent = (Sentence(f'Label: {label} describes document: {doc}'))
            embeddings = self.crazy_emb.embed(sent)
            label_emb = []
            for token in embeddings[0].tokens[2:2 + len(label.split(' '))]:
                label_emb.append(np.array(token.embedding.cpu()))

            label_embeddings.append(np.mean(label_emb, axis=0))

        label_embeddings = np.array(label_embeddings)
        return label_embeddings

    def single_doc_keyword_with_knn(self, doc):
        candidates, candidate_embeddings = self.get_candidates(doc)

        if len(candidates) <= 0:
            candidates2, candidate_embeddings2 = self.backup_candidates(doc)
            candidates.extend(candidates2)
            candidate_embeddings = np.vstack((candidate_embeddings, candidate_embeddings2))

        if self.sudo_keyphrase_embed and self.model_type == 'Trained' and self.pipeline != 'single':
            candidate_embeddings = self.sudo_keyphrase_embedding(candidates, doc)

        keyphrases = []
        keyphrases_weights = []

        if self.rank == 'classifier':
            weights = np.array(self.cl.predict_proba(candidate_embeddings))
            if type(self.cl) is ClassifierChain:
                weights = weights.transpose()
            elif type(self.cl) is MultiOutputClassifier:
                weights = weights[:, :, 1]
            else:
                weights = weights.transpose()
        elif self.rank == 'cosine':
            weights = cosine_similarity(candidate_embeddings, self.label_emb)
            weights = weights.transpose()
        elif self.rank == 'perdoc_cosine':
            label_embeddings = self.perdoc_label_embed(doc)
            weights = cosine_similarity(candidate_embeddings, label_embeddings)
            weights = weights.transpose()

        for label_index, label_weights in zip(range(len(self.label_names)), weights):
            top_indexes = label_weights.argsort()[:-(self.top_n + 1):-1]

            temp_weights = np.array([float(label_weights[index]) for index in top_indexes])
            temp_keyphrases = [candidates[indx] for indx in top_indexes]
            temp_weights = temp_weights[temp_weights > self.threshold]
            temp_keyphrases =  temp_keyphrases[:len(temp_weights)]
            if len(temp_weights) == 0:
                temp_weights = [float(label_weights[top_indexes[0]])]
                temp_keyphrases = [candidates[top_indexes[0]]]

            keyphrases.append(temp_keyphrases)
            keyphrases_weights.append(temp_weights)

        return keyphrases, keyphrases_weights
